{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring dimlpfidex with OmniXAI for heart attack prediction\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "Welcome to HES-Xplain, our interactive platform designed to facilitate explainable artificial intelligence (XAI) techniques. In this use case, we use the [Heart Attack Analysis & Prediction Dataset](https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset). This notebook is going to showcase how to use the MLXplain (OmniXAI) integration of [DimlpBT](https://hes-xplain.github.io/documentation/dimlpfidex/dimlp/dimlpbt/), [Fidex](https://hes-xplain.github.io/documentation/dimlpfidex/fidex/fidex/) & [FidexGloRules](https://hes-xplain.github.io/documentation/dimlpfidex/fidex/fidexglorules/).\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "    1. Observe a different use case where XAI can be used.\n",
    "    2. Understand how to pre-process data.\n",
    "    3. Understand how to use Dimlp and Fidex with OmniXAI.\n",
    "    4. Showcase the versatility of HES-Xplain using a different dataset.\n",
    "    5. Provide practical insights into applying Dimlp and Fidex to heart attack prediction through an interactive notebook.\n",
    "    6. Foster a community of XAI enthusiasts and practitioners.\n",
    "\n",
    "**Outline:**\n",
    "\n",
    "    1. Dataset and Problem Statement.\n",
    "    2. Load and pre-process the dataset.\n",
    "    3. Train the Model.\n",
    "    4. Local and global rules generation & OmniXAI dashboard display.\n",
    "    5. References.\n",
    "\n",
    "Through this use case, we aim to show the users the potential of Dimlp and Fidex as tools for transparent and interpretable classification. With HES-Xplain, we make XAI accessible, helping users build trust in their models and make informed decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Problem Statement\n",
    "The dataset we'll be working with is called the [Heart Attack Analysis & Prediction Dataset](https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset) and is accessible on [Kaggle](https://www.kaggle.com). It comprises 303 anonymized data records containing health information. In this notebook, our focus is predicting the risk of heart attack based on all given factors. By leveraging deep learning techniques and Fidex algorithms, we aim to not only achieve high classification performance but also gain insights into the attributes (pixels here) that contribute to the classification decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and pre-process the dataset\n",
    "Let's start by preprocessing the data. We categorize the cp attribtue and the classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib as pl\n",
    "from random import randint\n",
    "from omnixai_community.data.tabular import Tabular\n",
    "from omnixai_community.visualization.dashboard import Dashboard\n",
    "from omnixai_community.explainers.tabular.auto import TabularExplainer\n",
    "from mlxplain.explainers.tabular.specific.dimlpfidex import DimlpBTModel\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(\"heart.csv\")\n",
    "\n",
    "cp = pd.get_dummies(\n",
    "    dataset[\"cp\"], prefix=\"cp\", prefix_sep=\"_\", columns=[\"cp\"], dtype=\"int8\"\n",
    ")\n",
    "dataset = pd.concat([dataset.iloc[:, :2], cp, dataset.iloc[:, 3:]], axis=1)\n",
    "dataset = dataset.rename(\n",
    "    columns={\n",
    "        \"cp_0\": \"cp_typical\",\n",
    "        \"cp_1\": \"cp_atypical\",\n",
    "        \"cp_2\": \"cp_nonanginal \",\n",
    "        \"cp_3\": \"cp_asymptomatic\",\n",
    "    }\n",
    ")\n",
    "\n",
    "output = pd.get_dummies(\n",
    "    dataset[\"output\"], prefix=\"risk\", prefix_sep=\"_\", columns=[\"output\"], dtype=\"int8\"\n",
    ")\n",
    "\n",
    "dataset = pd.concat([dataset.iloc[:, :-1], output], axis=1)\n",
    "dataset = dataset.rename(columns={\"risk_0\": \"risk_no\", \"risk_1\": \"risk_yes\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must split the dataset into a train and test subsets and write feature names in a file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.sample(frac=1)\n",
    "split = int(dataset.shape[0] * 0.8)\n",
    "\n",
    "features = dataset.columns\n",
    "\n",
    "nb_classes = 2\n",
    "nb_features = len(features) - nb_classes\n",
    "\n",
    "train_dataset = Tabular(\n",
    "    data=dataset.iloc[:split,:]\n",
    ")\n",
    "\n",
    "test_dataset = Tabular(\n",
    "    data=dataset.iloc[split:,:]\n",
    ")\n",
    "\n",
    "root_dir = pl.Path(\"out\")\n",
    "root_dir.mkdir(parents=True, exist_ok=True)\n",
    "features_filename = \"attributes.txt\"\n",
    "\n",
    "with open(root_dir.joinpath(features_filename), \"w\") as file:\n",
    "    for feature in features:\n",
    "        file.write(f\"{feature}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Let's train the [DimlpBT](https://hes-xplain.github.io/documentation/dimlpfidex/dimlp/dimlpbt/) model with our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: verbose_console can't be used when used in notebooks for some reason\n",
    "\n",
    "model = DimlpBTModel(\n",
    "    root_dir,\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    nb_features,\n",
    "    nb_classes,\n",
    "    attributes_file=features_filename,\n",
    ")\n",
    "\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local and global rules generation & OmniXAI dashboard display\n",
    "The rule extraction must now be initialized and executed with Fidex & FidexGloRules to generate local & global explainations. Then, we display the explanations on the OmniXAI dashboard :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 10\n",
    "min_covering = 2\n",
    "max_failed_attempts = 15\n",
    "min_fidelity = 1.0\n",
    "lowest_fidelity_allowed = 0.9\n",
    "use_minimal_version = True\n",
    "\n",
    "explainer = TabularExplainer(\n",
    "    explainers=[\"fidex\", \"fidexGloRules\"],\n",
    "    data=train_dataset,\n",
    "    model=model,\n",
    "    mode=\"classification\",\n",
    "    params={\n",
    "        \"fidex\": {\n",
    "            \"max_iterations\": max_iterations,\n",
    "            \"attributes_file\": features_filename,\n",
    "            \"min_covering\": min_covering,\n",
    "            \"max_failed_attempts\": max_failed_attempts,\n",
    "            \"min_fidelity\": min_fidelity,\n",
    "            \"lowest_min_fidelity\": lowest_fidelity_allowed,\n",
    "        },\n",
    "        \"fidexGloRules\": {\n",
    "            \"heuristic\": 1,\n",
    "            \"with_fidexGlo\": True,\n",
    "            \"fidexGlo\": {\n",
    "                \"attributes_file\": features_filename,\n",
    "                \"with_minimal_version\": use_minimal_version,\n",
    "                \"max_iterations\": max_iterations,\n",
    "                \"min_covering\": min_covering,\n",
    "                \"covering_strategy\": True,\n",
    "                \"max_failed_attempts\": max_failed_attempts,\n",
    "                \"min_fidelity\": min_fidelity,\n",
    "                \"lowest_min_fidelity\": lowest_fidelity_allowed,\n",
    "            },\n",
    "            \"attributes_file\": features_filename,\n",
    "            \"nb_threads\": 4,\n",
    "            \"with_minimal_version\": use_minimal_version,\n",
    "            \"max_iterations\": max_iterations,\n",
    "            \"min_covering\": min_covering,\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "# predict with random sample\n",
    "sample_to_test = test_dataset[randint(0, len(test_dataset)-1)]\n",
    "\n",
    "local_explainations = explainer.explain(X=sample_to_test)\n",
    "global_explainations = explainer.explain_global()\n",
    "\n",
    "dashboard = Dashboard(\n",
    "    instances=sample_to_test,\n",
    "    local_explanations=local_explainations,\n",
    "    global_explanations=global_explainations,\n",
    "    class_names=features[-2:],\n",
    ")\n",
    "dashboard.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"Local Explaination\" tab, you can observe the sample randomly chosen along with the prediction made by the model.  The rule explaining the model's decision is displayed on the right. To see details about the rule, you can hover on the blue bar. \n",
    "\n",
    "In the \"Global Explaination\" tab, there is a bar chart with all the global rules computed. They are ranked by their covering size (number of samples covered).\n",
    "\n",
    "# References\n",
    "\n",
    "HES-XPLAIN: [website](https://hes-xplain.github.io/), [Github page](https://github.com/HES-XPLAIN)\n",
    "\n",
    "Dataset: [source](https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset)\n",
    "\n",
    "Dimlpfidex: [Github repository](https://github.com/HES-XPLAIN/dimlpfidex), [documentation](https://hes-xplain.github.io/documentation/overview/)\n",
    "\n",
    "Algorithms: [DimlpBT](https://hes-xplain.github.io/documentation/dimlpfidex/dimlp/dimlpbt/), [Fidex](https://hes-xplain.github.io/documentation/dimlpfidex/fidex/fidex/), [FidexGloRules](https://hes-xplain.github.io/documentation/dimlpfidex/fidex/fidexglorules/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
